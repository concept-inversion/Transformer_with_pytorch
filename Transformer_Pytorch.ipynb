{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch implementation of Transformer\n",
    "Code heavily inspired from https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec#d554."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from graphviz import Digraph\n",
    "import numpy as np\n",
    "import math\n",
    "import spacy\n",
    "#import en_core_web_sm\n",
    "#import fr_core_news_sm\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "from torchviz import make_dot\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenizer\n",
    "![alt text](sentence_token.png \"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "en = open('english.txt', encoding='utf-8').read().split('\\n')\n",
    "fr = open('french.txt', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "en_ = spacy.load('en_core_web_sm')\n",
    "fr_ = spacy.load('fr_core_news_sm')\n",
    "\n",
    "def tokenize_en(sentence):\n",
    "    return [tok.text for tok in en_.tokenizer(sentence)]\n",
    "def tokenize_fr(sentence):\n",
    "    return [tok.text for tok in fr_.tokenizer(sentence)]\n",
    "EN_TEXT = Field(tokenize=tokenize_en)\n",
    "FR_TEXT = Field(tokenize=tokenize_fr, init_token = \"<sos>\", eos_token = \"<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data into dataframe\n",
    "raw_data = {'English' : [line for line in en], 'French': [line for line in fr]}\n",
    "df = pd.DataFrame(raw_data, columns=[\"English\", \"French\"])\n",
    "# remove very long sentences and sentences where translations are \n",
    "# not of roughly equal length\n",
    "df['eng_len'] = df['English'].str.count(' ')\n",
    "df['fr_len'] = df['French'].str.count(' ')\n",
    "df = df.query('fr_len < 80 & eng_len < 80')\n",
    "df = df.query('fr_len < eng_len * 1.5 & fr_len * 1.5 > eng_len')\n",
    "df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train and validation set\n",
    "train, val = train_test_split(df,test_size=0.1)\n",
    "train.to_csv(\"train.csv\", index=False)\n",
    "val.to_csv(\"val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenized train and validation\n",
    "data_fields = [('English', EN_TEXT), ('French', FR_TEXT)]\n",
    "train,val = TabularDataset.splits(path='./', train='train.csv', validation='val.csv', format='csv', fields=data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index tokens\n",
    "FR_TEXT.build_vocab(train, val)\n",
    "EN_TEXT.build_vocab(train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = BucketIterator(train, batch_size=1, \\\n",
    " shuffle=False,repeat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch size:\n",
    "In each batch, the sentences have been transposed so they are descending vertically (important: we will need to transpose these again to work with the transformer). Each index represents a token (word), and each column represents a sentence. We have 10 columns, as 10 was the batch_size we specified.\n",
    "\n",
    "#### Determine batch size:\n",
    "Additionally, if your RAM can process say 1500 tokens each iteration, and your batch_size is 20, then only when you have batches of length 75 will you be utilising all the memory. An efficient batching mechanism would change the batch size depending on the sequence length to make sure around 1500 tokens were being processed each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2],\n",
      "        [10591],\n",
      "        [    3]])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(batch.French)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask Generation for input\n",
    "def input_mask_gen(EN_TEXT,input_seq):\n",
    "    #input_seq = batch.English.transpose(0,1)\n",
    "    input_pad = EN_TEXT.vocab.stoi['<pad>']\n",
    "    # creates mask with 0s wherever there is padding in the input\n",
    "    input_msk = (input_seq != input_pad).unsqueeze(1)\n",
    "    return input_msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask for target sequence\n",
    "def target_mask_gen(FR_TEXT,target_seq):\n",
    "    #target_seq = batch.French.transpose(0,1)\n",
    "    target_pad = FR_TEXT.vocab.stoi['<pad>']\n",
    "    target_msk = (target_seq != target_pad).unsqueeze(1)\n",
    "    size = target_seq.size(1) # get seq_len for matrix\n",
    "    nopeak_mask = np.triu(np.ones((1, size, size),dtype=int),k=1).astype('uint8')\n",
    "    nopeak_mask = Variable(torch.from_numpy(nopeak_mask) == 0)\n",
    "#     import ipdb;ipdb.set_trace()\n",
    "    target_msk = target_msk & nopeak_mask\n",
    "    return target_msk, target_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When each word is fed into the network, this code will perform a look-up and retrieve its embedding vector. These vectors will then be learnt as a parameters by the model, adjusted with each iteration of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define word embeddings\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed=nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self,x):\n",
    "        #print(list(self.embed.parameters()))\n",
    "       \n",
    "        #return x\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position Encoding \n",
    "  \n",
    "\n",
    "![alt text](position_encoding.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate positional encoding matrix with sin and cos function\n",
    "class positional_encoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 80):\n",
    "        super().__init__()\n",
    "        self.model_dim = d_model\n",
    "        # create a matrix for positional encoding\n",
    "        pos_encod= torch.zeros(max_seq_len, self.model_dim)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0,self.model_dim,2):\n",
    "                pos_encod[pos,i] = math.sin(pos/(10000**((2*i)/self.model_dim)))\n",
    "                pos_encod[pos,i+1] = math.cos(pos/(10000**((2*(i+1))/self.model_dim)))\n",
    "        pos_encod= pos_encod.unsqueeze(0)\n",
    "        self.register_buffer('pos_encod',pos_encod)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        # make embeddings\n",
    "        X = X * math.sqrt(self.model_dim)\n",
    "        # add constant\n",
    "        seq_len = X.size(1)\n",
    "#         import ipdb;ipdb.set_trace()\n",
    "        X = X + Variable(self.pos_encod[:,:seq_len], requires_grad=False)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Generation\n",
    "![alt text](Input_transformer.png \"Input black\")\n",
    "\n",
    "### MultiHead Attention\n",
    "![alt text](multihead.png \"Input black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-headed attention\n",
    "\n",
    "grads = {}\n",
    "results = {}\n",
    "def save_grad(name):\n",
    "    def hook(grad):\n",
    "        grads[name] = grad\n",
    "    return hook\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, model_dimension, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.model_dimension= model_dimension\n",
    "        self.keys_dim= model_dimension// heads\n",
    "        self.head= heads\n",
    "        \n",
    "        #query vector\n",
    "        self.q_linear= nn.Linear(model_dimension, model_dimension)\n",
    "        #value vector\n",
    "        self.v_linear= nn.Linear(model_dimension, model_dimension)\n",
    "        #key vector\n",
    "        self.k_linear= nn.Linear(model_dimension, model_dimension)\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "        \n",
    "        self.output= nn.Linear(model_dimension, model_dimension)\n",
    "        \n",
    "    def forward(self, query, keys, value, mask=None, name=None):\n",
    "        bs = query.size(0)\n",
    "        keys= self.k_linear(keys).view(bs, -1, self.head, self.keys_dim)\n",
    "        results['keys_'+name] = keys\n",
    "        keys.register_hook(save_grad('keys_grad_'+name))\n",
    "        query= self.q_linear(query).view(bs, -1, self.head, self.keys_dim)\n",
    "        results['query_'+name] = query\n",
    "        query.register_hook(save_grad('query_grad_'+name))\n",
    "        value= self.v_linear(value).view(bs, -1, self.head, self.keys_dim)\n",
    "        results['value_'+name]= value\n",
    "        value.register_hook(save_grad('value_grad_'+name))\n",
    "#         print('keys_'+name)\n",
    "#         print(keys)\n",
    "#         print('value_'+name)\n",
    "#         print(value)\n",
    "        keys= keys.transpose(1,2)\n",
    "        query= query.transpose(1,2)\n",
    "        value= value.transpose(1,2)\n",
    "        \n",
    "        attention_score= attention(query,keys,value,self.keys_dim,mask,self.dropout, name)        \n",
    "        results['attn_score_'+name] = attention_score\n",
    "        attention_score.register_hook(save_grad('attn_score_'+name))\n",
    "        # combine the result from all head\n",
    "        concat_result= attention_score.transpose(1,2).contiguous().view(bs, -1, self.model_dimension)\n",
    "        results['attn_concat_'+name] = concat_result\n",
    "        concat_result.register_hook(save_grad('attn_conct_'+name))        \n",
    "        # pass through a last layer to match the dimension (Multiply with Wo)\n",
    "        output= self.output(concat_result)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For encoder, \n",
    "#Dimension: Batch_size * seq_len * model_dimension\n",
    "# For Multi-head attention: (split into N heads)\n",
    "# Dimension: batch_size * N * seq_len * (model_dimension/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single attention\n",
    "![alt text](single_attention.png \"attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention: used by both encoder and decoder\n",
    "def attention(q, k, v, d_k, mask=None, dropout=None, name=None):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    results['score_'+name]=scores\n",
    "    scores.register_hook(save_grad('score_'+name))\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    results['scores_softmax_'+name]= scores\n",
    "    scores.register_hook(save_grad('score_softmax_'+name))\n",
    "    \n",
    "    if dropout is not None:\n",
    "        pass\n",
    "        #scores = dropout(scores)\n",
    "    \n",
    "    output = torch.matmul(scores, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define FeedForward \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,model_dimension, dim_forward=3, dropout= 0.1):\n",
    "        super().__init__()\n",
    "        self.linear_1= nn.Linear(model_dimension, dim_forward)\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "        self.linear_2= nn.Linear(dim_forward, model_dimension)\n",
    "    \n",
    "    def forward(self,X,name=\"None\"):\n",
    "        #print(\"Forward Layer\")\n",
    "        X= (F.relu(self.linear_1(X)))\n",
    "        results['x_linear_1_'+name] = X\n",
    "        #print(X)\n",
    "        #X.register_hook(save_grad('x_linear_1_'+name))\n",
    "        \n",
    "        X= self.linear_2(X)\n",
    "        results['x_linear_2_'+name] = X\n",
    "        #X.register_hook(save_grad('x_linear_2_'+name))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=results['x_linear_1_enc'].detach().numpy()\n",
    "#b=grads['x_linear_1_enc'].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "    def forward(self, x, name=None):\n",
    "#         print(\"++++++++++++++mean+++++++++++++++\")\n",
    "#         print(x.mean(dim=-1, keepdim=True))\n",
    "#         print(x.std(dim=-1, keepdim=True))\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        norm.register_hook(save_grad('norm_'+name))\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "![alt text](encoder.png \"attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "   \n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "       # print(\"Att crossed\")\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        #print(\"Encoder out\")\n",
    "        return x\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff= FeedForward(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout) \n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.norm_1 = Norm(d_model) \n",
    "        self.norm_2 = Norm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask):   \n",
    "        results['x'] = x\n",
    "        x2=x\n",
    "        #x2=self.norm_1(x,\"enc\")\n",
    "        results['x2']=x2\n",
    "        #x2.register_hook(save_grad('input_norm_out'))\n",
    "        results['input_norm'] = x2\n",
    "        \n",
    "        #x = x + self.dropout_1(self.attn(x2,x2,x2,mask,\"enc\"))\n",
    "        x = x + (self.attn(x2,x2,x2,mask,\"enc\"))\n",
    "        #x.register_hook(save_grad('enc_att_out'))\n",
    "        results['enc_attn_out'] = x\n",
    "        \n",
    "        xz=x\n",
    "        #xz= self.norm_2(x,\"enc\")\n",
    "        results['After_attn_norm'] = xz\n",
    "        #xz.register_hook(save_grad('after_norm'))\n",
    "        \n",
    "        x2 = x+ (self.ff(xz,'enc'))\n",
    "        results['enc_after_ff'] = x2\n",
    "        #x2.register_hook(save_grad('enc_after_ff'))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, model_dimension, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.normalize1= Norm(model_dimension)\n",
    "        self.normalize2= Norm(model_dimension)\n",
    "        self.normalize3= Norm(model_dimension)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttention(heads, model_dimension)\n",
    "        self.attn_2 = MultiHeadAttention(heads, model_dimension)\n",
    "        self.ff = FeedForward(model_dimension)\n",
    "        \n",
    "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "        x2 = self.normalize1(x)\n",
    "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "       # print(\"Dec passed 1\")\n",
    "        x2 = self.normalize2(x)\n",
    "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,src_mask))\n",
    "       # print(\"Dec passed 2\")\n",
    "        x2 = self.normalize3(x)\n",
    "        x = x + self.dropout_3(self.ff(x2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, model_dimension, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttention(heads, model_dimension)\n",
    "        self.attn_2 = MultiHeadAttention(heads, model_dimension)\n",
    "        self.ff = FeedForward(model_dimension)\n",
    "        \n",
    "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "        x2=x\n",
    "        x = x + (self.attn_1(x2, x2, x2, trg_mask,\"dec\"))\n",
    "        x = x + (self.attn_2(x2, e_outputs, e_outputs,src_mask,\"dec\"))\n",
    "        x = x + (self.ff(x2,\"dec\"))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dimension, N, heads):\n",
    "        super().__init__()\n",
    "        self.N= N\n",
    "        self.embed= Embedder(vocab_size, model_dimension)\n",
    "        self.position_encoder= positional_encoding(model_dimension)\n",
    "        self.layers= get_clones(EncoderLayer(model_dimension, heads), N)\n",
    "        self.normalized= Norm(model_dimension)\n",
    "        self.norm= Norm(model_dimension)\n",
    "    def forward(self, source, mask):\n",
    "        x= self.embed(source)\n",
    "        x= self.position_encoder(x)\n",
    "#         print(\"Encoder\")\n",
    "#         print(x)\n",
    "        results['input_encoding'] = x\n",
    "        #x.register_hook(save_grad('input_encod_encoder'))\n",
    "        for i in range(N):\n",
    "            #print(\"Encoder:\",i)\n",
    "            x= self.layers[i](x, mask)\n",
    "            results['enc_Ni_out'] = x\n",
    "            #x.register_hook(save_grad('enc_Ni_out'))\n",
    "        x_out= self.norm(x,\"enc\")\n",
    "        results['enc_out'] = x_out\n",
    "        x_out.register_hook(save_grad('enc_out'))\n",
    "        return x_out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dimension, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, model_dimension)\n",
    "        self.positional_encoder = positional_encoding(model_dimension)\n",
    "        self.layers = get_clones(DecoderLayer(model_dimension, heads), N)\n",
    "        self.normalized= Norm(model_dimension)\n",
    "    \n",
    "    def forward(self, target,\n",
    "                e_output, source_mask, target_mask):\n",
    "        #print(target)\n",
    "        x= self.embed(target)\n",
    "        x= self.positional_encoder(x)\n",
    "#         print(\"Decoder\")\n",
    "#         print(x)\n",
    "        for i in range(self.N):\n",
    "            #print(\"Decoder:\",i)\n",
    "            x= self.layers[i](x, e_output, source_mask, target_mask)\n",
    "        return x\n",
    "        #return self.normalized(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.encoder= Encoder(src_vocab, d_model,N,heads)\n",
    "        self.decoder= Decoder(trg_vocab, d_model,N,heads)\n",
    "        self.output= nn.Linear(d_model, trg_vocab)\n",
    "    def forward(self, source, target, source_mask, target_mask):\n",
    "        encoder_out= self.encoder(source, source_mask)\n",
    "        decoder_out= self.decoder(target, encoder_out, source_mask, target_mask)\n",
    "        output= self.output(decoder_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dimension= 2\n",
    "attention_heads= 1\n",
    "N = 1\n",
    "source_vocab= len(EN_TEXT.vocab)\n",
    "target_vocab= len(FR_TEXT.vocab)\n",
    "\n",
    "model= Transformer(source_vocab, target_vocab, model_dimension, N, attention_heads)\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# initialize the parameters\n",
    "for p in model.parameters():\n",
    "    if p.dim()>1:\n",
    "        nn.init.xavier_normal_(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9,0.98), eps= 1e-9)\n",
    "def train_model(epochs, print_every=1):\n",
    "    model.train()\n",
    "    start_all = time.time()\n",
    "    start = time.time()\n",
    "    temp = start\n",
    "    total_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        c = 2;\n",
    "        for i,batch in enumerate(train_iter):\n",
    "            # print(\"Batch:\",i)\n",
    "            c= c-1\n",
    "            if(c<1):\n",
    "                break;\n",
    "            start = time.time()\n",
    "            src= batch.English.transpose(0,1)\n",
    "#             print(\"src\")\n",
    "#             print(src)\n",
    "            target= batch.French.transpose(0,1)\n",
    "            # Last word to predict\n",
    "            #import ipdb;ipdb.set_trace()\n",
    "            target_input= target[:,:-1]\n",
    "            targets = target[:, 1:].contiguous().view(-1)\n",
    "            source_mask= input_mask_gen(EN_TEXT,src)\n",
    "            target_mask,target_pad= target_mask_gen(FR_TEXT,target_input)\n",
    "            #z=make_dot(model(src, target_input, source_mask, target_mask),params=dict(model.named_parameters()))           \n",
    "            #z.view()\n",
    "            st= time.time()\n",
    "            preds= model(src, target_input, source_mask, target_mask)\n",
    "            #import ipdb;ipdb.set_trace()\n",
    "            print(\"forward time:\",time.time()-st)\n",
    "            #print(\"done\")\n",
    "            optimizer.zero_grad()\n",
    "            loss= F.cross_entropy(preds.view(-1, preds.size(-1)),targets, ignore_index=target_pad)\n",
    "            star= time.time()\n",
    "            loss.backward()\n",
    "            #import ipdb;ipdb.set_trace()\n",
    "            print(\"gradient computation time:\",time.time()-star)\n",
    "            start = time.time()\n",
    "            optimizer.step()\n",
    "            print(\"update time:\",time.time()-start)\n",
    "            total_loss += loss.data\n",
    "            if (i + 1) % print_every == 0:\n",
    "                loss_avg = total_loss / print_every\n",
    "                print(\"time = %dm, epoch %d, iter = %d, loss = %.3f,\\\n",
    "                %ds per %d iters\" % ((time.time() - start) // 60,\n",
    "                epoch + 1, i + 1, loss_avg, time.time() - temp,\n",
    "                print_every))\n",
    "                total_loss = 0\n",
    "                temp = time.time()\n",
    "    print(\"Overall time: \",time.time()-start_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "forward time: 0.007552146911621094\n",
      "gradient computation time: 0.0037813186645507812\n",
      "update time: 0.005404949188232422\n",
      "time = 0m, epoch 1, iter = 1, loss = 10.413,                0s per 1 iters\n",
      "Overall time:  0.022859573364257812\n"
     ]
    }
   ],
   "source": [
    "train_model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_encoding': tensor([[[-0.0184,  0.9761]]], grad_fn=<AddBackward0>),\n",
       " 'x': tensor([[[-0.0184,  0.9761]]], grad_fn=<AddBackward0>),\n",
       " 'x2': tensor([[[-0.0184,  0.9761]]], grad_fn=<AddBackward0>),\n",
       " 'input_norm': tensor([[[-0.0184,  0.9761]]], grad_fn=<AddBackward0>),\n",
       " 'keys_enc': tensor([[[[-0.4094, -0.0295]]]], grad_fn=<ViewBackward>),\n",
       " 'query_enc': tensor([[[[0.2516, 1.5710]]]], grad_fn=<ViewBackward>),\n",
       " 'value_enc': tensor([[[[ 1.4015, -0.2275]]]], grad_fn=<ViewBackward>),\n",
       " 'score_enc': tensor([[[[-0.1056]]]], grad_fn=<DivBackward0>),\n",
       " 'scores_softmax_enc': tensor([[[[1.]]]], grad_fn=<SoftmaxBackward>),\n",
       " 'attn_score_enc': tensor([[[[ 1.4015, -0.2275]]]], grad_fn=<UnsafeViewBackward>),\n",
       " 'attn_concat_enc': tensor([[[ 1.4015, -0.2275]]], grad_fn=<ViewBackward>),\n",
       " 'enc_attn_out': tensor([[[-0.1214,  1.0725]]], grad_fn=<AddBackward0>),\n",
       " 'After_attn_norm': tensor([[[-0.1214,  1.0725]]], grad_fn=<AddBackward0>),\n",
       " 'x_linear_1_enc': tensor([[[0., 0., 0.]]], grad_fn=<ReluBackward0>),\n",
       " 'x_linear_2_enc': tensor([[[0.2319, 0.4315]]], grad_fn=<AddBackward0>),\n",
       " 'enc_after_ff': tensor([[[0.1105, 1.5040]]], grad_fn=<AddBackward0>),\n",
       " 'enc_Ni_out': tensor([[[-0.1214,  1.0725]]], grad_fn=<AddBackward0>),\n",
       " 'enc_out': tensor([[[-0.7071,  0.7071]]], grad_fn=<AddBackward0>),\n",
       " 'keys_dec': tensor([[[[-1.1464,  0.4681]]]], grad_fn=<ViewBackward>),\n",
       " 'query_dec': tensor([[[[-0.7453, -1.4407]],\n",
       " \n",
       "          [[-0.1303, -0.9524]]]], grad_fn=<ViewBackward>),\n",
       " 'value_dec': tensor([[[[-0.5423,  0.6204]]]], grad_fn=<ViewBackward>),\n",
       " 'score_dec': tensor([[[[ 0.1273],\n",
       "           [-0.2097]]]], grad_fn=<DivBackward0>),\n",
       " 'scores_softmax_dec': tensor([[[[1.],\n",
       "           [1.]]]], grad_fn=<SoftmaxBackward>),\n",
       " 'attn_score_dec': tensor([[[[-0.5423,  0.6204],\n",
       "           [-0.5423,  0.6204]]]], grad_fn=<UnsafeViewBackward>),\n",
       " 'attn_concat_dec': tensor([[[-0.5423,  0.6204],\n",
       "          [-0.5423,  0.6204]]], grad_fn=<ViewBackward>),\n",
       " 'x_linear_1_dec': tensor([[[0.0447, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.2190]]], grad_fn=<ReluBackward0>),\n",
       " 'x_linear_2_dec': tensor([[[-0.0328,  0.0672],\n",
       "          [-0.2020, -0.0327]]], grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.1105241, 1.504023 ]]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder forward\n",
    "\n",
    "import numpy as np\n",
    "#from scipy.special import softmax\n",
    "\n",
    "\n",
    "# In reality, for ML models, each column should represent one sequence. Hence, we need to transpose two times\n",
    "e_dim= 2;\n",
    "seq_len= 4;\n",
    "\n",
    "def normalize():\n",
    "    pass\n",
    "def normback():\n",
    "    pass\n",
    "\n",
    "def softmax1(x):\n",
    "    #return softmax(Z,axis=1)\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def dropout(p,dim):\n",
    "    u1 = np.random.binomial(1, p, size=dim.shape)\n",
    "    return (p*u1)\n",
    "\n",
    "def Relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "# Input\n",
    "X= results['input_encoding'].detach().numpy()\n",
    "#import ipdb; ipdb.set_trace()\n",
    "# Normalize\n",
    "X_norm= results['input_norm'].detach().numpy()\n",
    "\n",
    "X_norm= X_norm[0]\n",
    "# Define Wq,bq; Wk, bk; Wv, bv\n",
    "Wq= model.encoder.layers[0].attn.q_linear.weight.detach().numpy()\n",
    "bq= model.encoder.layers[0].attn.q_linear.bias.detach().numpy()\n",
    "\n",
    "Wk= model.encoder.layers[0].attn.k_linear.weight.detach().numpy()\n",
    "bk= model.encoder.layers[0].attn.k_linear.bias.detach().numpy()\n",
    "\n",
    "Wv= model.encoder.layers[0].attn.v_linear.weight.detach().numpy()\n",
    "bv= model.encoder.layers[0].attn.v_linear.bias.detach().numpy()\n",
    "\n",
    "Q = np.dot(Wq,X_norm.transpose()).transpose() + bq\n",
    "#print(Q)\n",
    "K = np.dot(Wk,X_norm.transpose()).transpose() + bk\n",
    "#print(K)\n",
    "V = np.dot(Wv,X_norm.transpose()).transpose() + bv\n",
    "Z= np.dot(Q,np.transpose(K))/np.sqrt(e_dim)\n",
    "\n",
    "Z_res= softmax1(Z)\n",
    "\n",
    "Z_out = np.dot(Z_res,V)\n",
    "\n",
    "ZZ= (Z_out)\n",
    "# Define output parameter for attention\n",
    "Wo= model.encoder.layers[0].attn.output.weight.detach().numpy()\n",
    "bo= model.encoder.layers[0].attn.output.bias.detach().numpy()\n",
    "attn_out= np.dot(Wo, ZZ.transpose()).transpose() + bo \n",
    "\n",
    "Zo= attn_out + X\n",
    "\n",
    "Zoo= (Zo)\n",
    "\n",
    "# Normalized input before FC layer\n",
    "Zoo_norm= results['After_attn_norm'].detach().numpy()\n",
    "\n",
    "\n",
    "# Define weights for linear model\n",
    "w1= model.encoder.layers[0].ff.linear_1.weight.detach().numpy().transpose()\n",
    "b1= model.encoder.layers[0].ff.linear_1.bias.detach().numpy()\n",
    "\n",
    "w2= model.encoder.layers[0].ff.linear_2.weight.detach().numpy().transpose()\n",
    "b2= model.encoder.layers[0].ff.linear_2.bias.detach().numpy()\n",
    "\n",
    "Z_ff_1 = np.dot(Zoo_norm, w1) + b1\n",
    "\n",
    "Z_ff_1_relu = Relu(Z_ff_1)\n",
    "Z_ff_2 = np.dot(Z_ff_1_relu, w2) + b2\n",
    "\n",
    "H = Z_ff_2 + Zoo\n",
    "\n",
    "#E_out= normalize(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderLayer(\n",
       "  (attn_1): MultiHeadAttention(\n",
       "    (q_linear): Linear(in_features=2, out_features=2, bias=True)\n",
       "    (v_linear): Linear(in_features=2, out_features=2, bias=True)\n",
       "    (k_linear): Linear(in_features=2, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (output): Linear(in_features=2, out_features=2, bias=True)\n",
       "  )\n",
       "  (attn_2): MultiHeadAttention(\n",
       "    (q_linear): Linear(in_features=2, out_features=2, bias=True)\n",
       "    (v_linear): Linear(in_features=2, out_features=2, bias=True)\n",
       "    (k_linear): Linear(in_features=2, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (output): Linear(in_features=2, out_features=2, bias=True)\n",
       "  )\n",
       "  (ff): FeedForward(\n",
       "    (linear_1): Linear(in_features=2, out_features=3, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear_2): Linear(in_features=3, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder forward\n",
    "\n",
    "import numpy as np\n",
    "#from scipy.special import softmax\n",
    "\n",
    "\n",
    "# In reality, for ML models, each column should represent one sequence. Hence, we need to transpose two times\n",
    "e_dim= 2;\n",
    "seq_len= 4;\n",
    "\n",
    "def normalize():\n",
    "    pass\n",
    "def normback():\n",
    "    pass\n",
    "\n",
    "def softmax1(x):\n",
    "    #return softmax(Z,axis=1)\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def dropout(p,dim):\n",
    "    u1 = np.random.binomial(1, p, size=dim.shape)\n",
    "    return (p*u1)\n",
    "\n",
    "def Relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "# Input\n",
    "X= results['input_encoding'].detach().numpy()\n",
    "#import ipdb; ipdb.set_trace()\n",
    "# Normalize\n",
    "X_norm= results['input_norm'].detach().numpy()\n",
    "\n",
    "X_norm= X_norm[0]\n",
    "# Define Wq,bq; Wk, bk; Wv, bv\n",
    "Wq= model.decoder.layers[0].attn_1.q_linear.weight.detach().numpy()\n",
    "bq= model.decoder.layers[0].attn_1.q_linear.bias.detach().numpy()\n",
    "\n",
    "Wk= model.decoder.layers[0].attn_1.k_linear.weight.detach().numpy()\n",
    "bk= model.decoder.layers[0].attn_1.k_linear.bias.detach().numpy()\n",
    "\n",
    "Wv= model.decoder.layers[0].attn_1.v_linear.weight.detach().numpy()\n",
    "bv= model.decoder.layers[0].attn_1.v_linear.bias.detach().numpy()\n",
    "\n",
    "Q = np.dot(Wq,X_norm.transpose()).transpose() + bq\n",
    "#print(Q)\n",
    "K = np.dot(Wk,X_norm.transpose()).transpose() + bk\n",
    "#print(K)\n",
    "V = np.dot(Wv,X_norm.transpose()).transpose() + bv\n",
    "Z= np.dot(Q,np.transpose(K))/np.sqrt(e_dim)\n",
    "\n",
    "Z_res= softmax1(Z)\n",
    "\n",
    "Z_out = np.dot(Z_res,V)\n",
    "\n",
    "ZZ= (Z_out)\n",
    "\n",
    "# Define output parameter for attention\n",
    "Wo= model.decoder.layers[0].attn_1.output.weight.detach().numpy()\n",
    "bo= model.decoder.layers[0].attn_1.output.bias.detach().numpy()\n",
    "attn_out= np.dot(Wo, ZZ.transpose()).transpose() + bo \n",
    "\n",
    "Zo= attn_out + X\n",
    "\n",
    "Zoo= (Zo)\n",
    "\n",
    "# attention 2\n",
    "\n",
    "Wq_2= model.decoder.layers[0].attn_2.q_linear.weight.detach().numpy()\n",
    "bq_2= model.decoder.layers[0].attn_2.q_linear.bias.detach().numpy()\n",
    "\n",
    "Wk_2= model.decoder.layers[0].attn_2.k_linear.weight.detach().numpy()\n",
    "bk_2= model.decoder.layers[0].attn_2.k_linear.bias.detach().numpy()\n",
    "\n",
    "Wv_2= model.decoder.layers[0].attn_2.v_linear.weight.detach().numpy()\n",
    "bv_2= model.decoder.layers[0].attn_2.v_linear.bias.detach().numpy()\n",
    "\n",
    "\n",
    "Q_2 = np.dot(Wq_2,ZZ.transpose()).transpose() + bq_2\n",
    "#print(Q)\n",
    "K_2 = np.dot(Wk_2,ZZ.transpose()).transpose() + bk_2\n",
    "#print(K)\n",
    "V_2 = np.dot(Wv,ZZ.transpose()).transpose() + bv_2\n",
    "Z_2= np.dot(Q_2,np.transpose(K_2))/np.sqrt(e_dim)\n",
    "\n",
    "Z_res_2= softmax1(Z_2)\n",
    "\n",
    "Z_out_2 = np.dot(Z_res_2,V_2)\n",
    "\n",
    "ZZ_2= (Z_out_2)\n",
    "\n",
    "# Define output parameter for attention\n",
    "Wo_2= model.decoder.layers[0].attn_2.output.weight.detach().numpy()\n",
    "bo_2= model.decoder.layers[0].attn_2.output.bias.detach().numpy()\n",
    "attn_out_2= np.dot(Wo_2, ZZ_2.transpose()).transpose() + bo_2 \n",
    "\n",
    "Zo_2= attn_out_2 + Zoo\n",
    "\n",
    "Zoo_2= (Zo_2)\n",
    "\n",
    "# Normalized input before FC layer\n",
    "# Zoo_norm= results['After_attn_norm'].detach().numpy()\n",
    "\n",
    "Zoo_norm= Zoo_2\n",
    "# Define weights for linear model\n",
    "w1= model.encoder.layers[0].ff.linear_1.weight.detach().numpy().transpose()\n",
    "b1= model.encoder.layers[0].ff.linear_1.bias.detach().numpy()\n",
    "\n",
    "w2= model.encoder.layers[0].ff.linear_2.weight.detach().numpy().transpose()\n",
    "b2= model.encoder.layers[0].ff.linear_2.bias.detach().numpy()\n",
    "\n",
    "Z_ff_1 = np.dot(Zoo_norm, w1) + b1\n",
    "\n",
    "Z_ff_1_relu = Relu(Z_ff_1)\n",
    "Z_ff_2 = np.dot(Z_ff_1_relu, w2) + b2\n",
    "\n",
    "H = Z_ff_2 + Zoo\n",
    "\n",
    "#E_out= normalize(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.9742077 , -0.7036147 ],\n",
       "       [-0.90872276,  0.51572317],\n",
       "       [ 0.26548168, -0.16032153]], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= results['x'].detach().numpy()\n",
    "np.mean(x,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(x,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = model.encoder.layers[0].norm_1.alpha.detach().numpy()\n",
    "beta = model.encoder.layers[0].norm_1.bias.detach().numpy()\n",
    "eps = model.encoder.layers[0].norm_1.eps\n",
    "x= results['x'].detach().numpy().reshape(9,2)\n",
    "batchnorm_forward(x,gamma,beta,eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.array([[0.7004],\n",
    "         [0.1070],\n",
    "         [0.0727],\n",
    "         [0.6342],\n",
    "         [1.2626],\n",
    "         [1.3580],\n",
    "         [0.9090],\n",
    "         [0.2514],\n",
    "         [0.0237]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std.shape\n",
    "#self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    " #       / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up=x-np.mean(x,axis=-1).reshape(9,1)\n",
    "down = std +eps\n",
    "(gamma * up/down) + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up/down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_forward(x, gamma, beta, eps):\n",
    "    \n",
    "    #import ipdb;ipdb.set_trace()\n",
    "    N, D = x.shape\n",
    "    #step1: calculate mean\n",
    "    mu = 1./N * np.sum(x, axis = -1).reshape(9,1)\n",
    "    #step2: subtract mean vector of every trainings example\n",
    "    xmu = x - mu\n",
    "      #step3: following the lower branch - calculation denominator\n",
    "    sq = xmu ** 2\n",
    "      #step4: calculate variance\n",
    "    var = 1./N * np.sum(sq, axis = -1).reshape(9,1)\n",
    "      #step5: add eps for numerical stability, then sqrt\n",
    "    sqrtvar = np.sqrt(var + eps)\n",
    "      #step6: invert sqrtwar\n",
    "    ivar = 1./sqrtvar\n",
    "      #step7: execute normalization\n",
    "    xhat = xmu * ivar\n",
    "      #step8: Nor the two transformation steps\n",
    "    gammax = gamma * xhat\n",
    "      #step9\n",
    "    out = gammax + beta\n",
    "      #store intermediate\n",
    "    cache = (xhat,gamma,xmu,ivar,sqrtvar,var,eps)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def batchnorm_backward(dout, cache):\n",
    "\n",
    "  #unfold the variables stored in cache\n",
    "  xhat,gamma,xmu,ivar,sqrtvar,var,eps = cache\n",
    "  #get the dimensions of the input/output\n",
    "  N,D = dout.shape\n",
    "  #step9\n",
    "  dbeta = np.sum(dout, axis=0)\n",
    "  dgammax = dout #not necessary, but more understandable\n",
    "  #step8\n",
    "  dgamma = np.sum(dgammax*xhat, axis=0)\n",
    "  dxhat = dgammax * gamma\n",
    "\n",
    "  #step7\n",
    "  divar = np.sum(dxhat*xmu, axis=0)\n",
    "  dxmu1 = dxhat * ivar\n",
    "  #step6\n",
    "  dsqrtvar = -1. /(sqrtvar**2) * divar\n",
    "  #step5\n",
    "  dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar\n",
    "  #step4\n",
    "  dsq = 1. /N * np.ones((N,D)) * dvar\n",
    "  #step3\n",
    "  dxmu2 = 2 * xmu * dsq\n",
    "  #step2\n",
    "  dx1 = (dxmu1 + dxmu2)\n",
    "  dmu = -1 * np.sum(dxmu1+dxmu2, axis=0)\n",
    "  #step1\n",
    "  dx2 = 1. /N * np.ones((N,D)) * dmu\n",
    "  #step0\n",
    "  dx = dx1 + dx2\n",
    "  return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# H gradients\n",
    "D_H = grads['enc_Ni_out'].numpy().reshape(X.shape[1],e_dim)\n",
    "\n",
    "# FC-2 Layer\n",
    "D_Z_ff_2 = D_H\n",
    "dw2= np.dot(D_Z_ff_2.transpose(), Z_ff_1_relu)\n",
    "db2= np.sum(D_Z_ff_2,axis=0)\n",
    "\n",
    "# Relu\n",
    "D_Z_ff_1_relu = Relu(np.dot(D_Z_ff_2 , w2.transpose()))\n",
    "\n",
    "#FC-1 Layer\n",
    "D_Z_ff_1 = D_Z_ff_1_relu\n",
    "dw1= np.dot(D_Z_ff_1.transpose(), Zoo)\n",
    "db1= np.sum(D_Z_ff_1, axis=0)\n",
    "\n",
    "# Normalize Gradients\n",
    "D_Zoo= np.dot(D_Z_ff_1, w1.transpose()) \n",
    "\n",
    "#gradients for Normalization \n",
    "D_Zo= (D_Zoo)\n",
    "\n",
    "# Attention output layer\n",
    "D_Z_attn_out= D_Zo\n",
    "dwo= np.dot(D_Z_attn_out.transpose(), ZZ)\n",
    "dbo= np.sum(D_Z_attn_out, axis=0)  \n",
    "\n",
    "# Gradient of combination\n",
    "D_ZZ= np.dot(D_Z_attn_out, Wo)\n",
    "\n",
    "# Score computation\n",
    "D_Z_out= D_ZZ\n",
    "D_Z= np.dot(D_Z_out.transpose(),V)\n",
    "D_V= np.dot(D_Z_out.transpose(),Z)   # *********\n",
    "\n",
    "# Softmax backprop\n",
    "D_Z_x= np.diag(Z) - np.dot(Z,Z.transpose()) #***********\n",
    "\n",
    "# Query and Key\n",
    "D_Q=  np.dot(D_Z_x, K)/np.sqrt(e_dim)\n",
    "D_K=  np.dot(D_Z_x.transpose(),Q)/np.sqrt(e_dim)\n",
    "\n",
    "# Gradient for V\n",
    "D_Wv= np.dot(D_V, X_norm)\n",
    "D_bv= np.sum(D_V,axis=0)\n",
    "D_norm_x_v= np.dot(D_V.transpose(), Wv)\n",
    "\n",
    "# Gradient for Q\n",
    "D_Wq= np.dot(D_Q.transpose(), X_norm)\n",
    "D_bq= np.sum(D_Q,axis=0)\n",
    "D_norm_x_q= np.dot(D_Q, Wq)\n",
    "\n",
    "# Gradient for K\n",
    "D_Wk= np.dot(D_K.transpose(), X_norm)\n",
    "D_bk= np.sum(D_K,axis=0)\n",
    "D_norm_x_k= np.dot(D_K, Wk)\n",
    "\n",
    "#**************If we want to update the embeddings **********************\n",
    "# SUm of all D_norm_X\n",
    "D_norm = D_norm_x_k + D_norm_x_q + D_norm_x_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(D_Z_ff_2,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D_Wk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['x'].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=model.encoder.layers[0].norm_1.alpha.detach().numpy()\n",
    "bias=model.encoder.layers[0].norm_1.bias.detach().numpy()\n",
    "eps=model.encoder.layers[0].norm_1.eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out,cache=batchnorm_forward(results['x'].detach().numpy().reshape(9,2),alpha,bias,eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=results['x2'].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= torch.tensor([[ 0.0172,  1.0966],\n",
    "         [ 0.9420,  0.6059],\n",
    "         [ 1.0191, -0.4586],\n",
    "         [ 0.1643, -1.1157],\n",
    "         [-0.8335, -0.7352],\n",
    "         [-1.0699,  0.3336],\n",
    "         [-0.0000,  0.0000],\n",
    "         [ 0.7393,  0.8166],\n",
    "         [ 1.0900, -0.1723],\n",
    "         [ 0.4438, -1.0032],\n",
    "         [-0.5981, -0.9284],\n",
    "         [-1.1156,  0.0234],\n",
    "         [-0.6117,  0.0000],\n",
    "         [ 0.4853,  1.0180]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[-1.0000,  1.0000],\n",
    "         [ 1.0000, -1.0000],\n",
    "         [ 1.0000, -1.0000],\n",
    "         [ 1.0000, -1.0000],\n",
    "         [-1.0000,  1.0000],\n",
    "         [-1.0000,  1.0000],\n",
    "         [ 0.0000,  0.0000],\n",
    "         [-1.0000,  1.0000],\n",
    "         [ 1.0000, -1.0000],\n",
    "         [ 1.0000, -1.0000],\n",
    "         [ 1.0000, -1.0000],\n",
    "         [-1.0000,  1.0000],\n",
    "         [-1.0000,  1.0000],\n",
    "         [-1.0000,  1.0000]]], grad_fn=<NativeLayerNormBackward>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [1,1]\n",
    "b= [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.layers[0].norm_1(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
